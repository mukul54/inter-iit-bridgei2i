{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inter-IIT_transliteration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urv9DhLk0WiQ"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFKZvLDhEunO"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "import time\r\n",
        "import math"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "wfIXvDvcFwR0",
        "outputId": "749e8200-c888-4d6a-b518-870389d7599c"
      },
      "source": [
        "data = pd.read_csv(\"hien15k.csv\")\r\n",
        "data = data.drop(['Unnamed: 0'], axis=1)\r\n",
        "# data.columns = ['hi-en','hi']\r\n",
        "data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hi-en</th>\n",
              "      <th>hi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>suna</td>\n",
              "      <td>सुना</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tha</td>\n",
              "      <td>था</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dekha</td>\n",
              "      <td>देखा</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>na</td>\n",
              "      <td>न</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>samajha</td>\n",
              "      <td>समझा</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15323</th>\n",
              "      <td>brownlo</td>\n",
              "      <td>ब्राउनलो</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15324</th>\n",
              "      <td>roda</td>\n",
              "      <td>रोडा</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15325</th>\n",
              "      <td>shymaleshwor</td>\n",
              "      <td>स्यामलेश्वर</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15326</th>\n",
              "      <td>leonard</td>\n",
              "      <td>लियोनार्ड</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15327</th>\n",
              "      <td>gurudwar</td>\n",
              "      <td>गुरूद्वारा</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15328 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              hi-en           hi\n",
              "0              suna         सुना\n",
              "1               tha           था\n",
              "2             dekha         देखा\n",
              "3                na            न\n",
              "4           samajha         समझा\n",
              "...             ...          ...\n",
              "15323       brownlo     ब्राउनलो\n",
              "15324          roda         रोडा\n",
              "15325  shymaleshwor  स्यामलेश्वर\n",
              "15326       leonard    लियोनार्ड\n",
              "15327      gurudwar   गुरूद्वारा\n",
              "\n",
              "[15328 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKmR36R7GJln"
      },
      "source": [
        "data = data.drop_duplicates(subset=['hi-en'], ignore_index=True)\r\n",
        "data = data.dropna()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d2jwOsVGBA-",
        "outputId": "180471f8-1dc2-4071-b3f7-04035855d2b4"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15327"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xEJNyD5Le8r",
        "outputId": "807c009e-154b-4a94-e6e7-901e3e1a4340"
      },
      "source": [
        "sample = data['hi'][0]\r\n",
        "print(sample)\r\n",
        "print([c for c in sample])\r\n",
        "print(\"\".join([c for c in sample]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "सुना\n",
            "['स', 'ु', 'न', 'ा']\n",
            "सुना\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8Ge3v1O0mmc"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKU5jLxR1QJT"
      },
      "source": [
        "def clear_punk(text):\r\n",
        "  punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\",\r\n",
        "                 \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', \r\n",
        "                 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '!':' '}\r\n",
        "  punct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \r\n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \r\n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \r\n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \r\n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\r\n",
        "\r\n",
        "  for p in punct_mapping:\r\n",
        "    text = text.replace(p, punct_mapping[p])\r\n",
        "\r\n",
        "  for p in punct:\r\n",
        "    text = text.replace(p,' ')\r\n",
        "  \r\n",
        "  return text"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKxTl6MU0rVB"
      },
      "source": [
        "def cleaner(data):\r\n",
        "  hien = data['hi-en']\r\n",
        "  hi = data['hi']\r\n",
        "\r\n",
        "  hi = [clear_punk(text) for text in hi]\r\n",
        "  hien = [clear_punk(text) for text in hien]\r\n",
        "\r\n",
        "  data['hi-en'] = hien\r\n",
        "  data['hi'] = hi"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZJRmWu33frC"
      },
      "source": [
        "cleaner(data)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "1MFZsd0r3kw5",
        "outputId": "f737f072-76e8-4720-f1b6-0f8396c9c745"
      },
      "source": [
        "data"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hi-en</th>\n",
              "      <th>hi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>suna</td>\n",
              "      <td>सुना</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tha</td>\n",
              "      <td>था</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dekha</td>\n",
              "      <td>देखा</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>na</td>\n",
              "      <td>न</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>samajha</td>\n",
              "      <td>समझा</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15323</th>\n",
              "      <td>brownlo</td>\n",
              "      <td>ब्राउनलो</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15324</th>\n",
              "      <td>roda</td>\n",
              "      <td>रोडा</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15325</th>\n",
              "      <td>shymaleshwor</td>\n",
              "      <td>स्यामलेश्वर</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15326</th>\n",
              "      <td>leonard</td>\n",
              "      <td>लियोनार्ड</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15327</th>\n",
              "      <td>gurudwar</td>\n",
              "      <td>गुरूद्वारा</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15327 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              hi-en           hi\n",
              "0              suna         सुना\n",
              "1               tha           था\n",
              "2             dekha         देखा\n",
              "3                na            न\n",
              "4           samajha         समझा\n",
              "...             ...          ...\n",
              "15323       brownlo     ब्राउनलो\n",
              "15324          roda         रोडा\n",
              "15325  shymaleshwor  स्यामलेश्वर\n",
              "15326       leonard    लियोनार्ड\n",
              "15327      gurudwar   गुरूद्वारा\n",
              "\n",
              "[15327 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xKmH9CnQ7Vw"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxUZXS3wQ9MP",
        "outputId": "dfaae11f-7769-4791-c3e8-8d5ac7b6736e"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "import torchtext as tt\r\n",
        "from torchtext.vocab import Vocab\r\n",
        "\r\n",
        "from torch.nn.utils.rnn import pad_sequence\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "print(f\"Torch Version: {torch.__version__}\")\r\n",
        "print(f\"TorchText Version: {tt.__version__}\")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch Version: 1.8.0+cu101\n",
            "TorchText Version: 0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJE9ViZTNp04"
      },
      "source": [
        "def tokenize(word):\r\n",
        "  '''\r\n",
        "      Tokenize the words\r\n",
        "  '''\r\n",
        "  return [c for c in word.lower()]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u0qUilYRt_N"
      },
      "source": [
        "def build_vocab(data,min_freq ,tokenizer):\r\n",
        "  counter = Counter()\r\n",
        "  for word in data:\r\n",
        "    counter.update(tokenizer(word))\r\n",
        "  return Vocab(counter, min_freq=min_freq , specials=('<unk>', '<pad>', '<sos>', '<eos>'))"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1zl7JWTRE-l"
      },
      "source": [
        "hi_char_vocab =  build_vocab(data['hi'], min_freq=1 ,tokenizer = tokenize)\r\n",
        "hi_en_char_vocab = build_vocab(data['hi-en'],min_freq=75,tokenizer= tokenize)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy6VJSWaO5cT",
        "outputId": "c711b3fe-e232-43a1-889e-50366144f165"
      },
      "source": [
        "print(len(hi_char_vocab))\r\n",
        "print(len(hi_en_char_vocab))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "109\n",
            "31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOHAForoSDRM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fcfb8ed-85aa-47dc-929c-d1a274c658fa"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "device"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MesIa8eZPPbQ"
      },
      "source": [
        "def create_dataset(data, tokenizer, src_vocab, tar_vocab):\r\n",
        "  src_raw_iter = iter(data['hi-en'])\r\n",
        "  trg_raw_iter = iter(data['hi'])\r\n",
        "\r\n",
        "  data = []\r\n",
        "\r\n",
        "  for raw_src, raw_trg in zip(src_raw_iter, trg_raw_iter):\r\n",
        "    src_tensor = torch.tensor([src_vocab[tok] for tok in tokenizer(raw_src)], dtype=torch.long)\r\n",
        "    trg_tensor = torch.tensor([tar_vocab[tok] for tok in tokenizer(raw_trg)], dtype=torch.long)\r\n",
        "\r\n",
        "    data.append((src_tensor,trg_tensor))\r\n",
        "\r\n",
        "  return data"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_XRQO9-VqR4"
      },
      "source": [
        "dataset = create_dataset(data, tokenize, hi_en_char_vocab, hi_char_vocab)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0r7dojUWFIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed74148-a9ac-4c90-d2dc-01aa02817a89"
      },
      "source": [
        "PAD_IDX = hi_char_vocab[\"<pad>\"]\r\n",
        "SOS_IDX = hi_char_vocab[\"<sos>\"]\r\n",
        "EOS_IDX = hi_char_vocab[\"<eos>\"]\r\n",
        "\r\n",
        "print(f\"pad index: {PAD_IDX}\")\r\n",
        "print(f\"sos index: {SOS_IDX}\")\r\n",
        "print(f\"eos index: {EOS_IDX}\")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pad index: 1\n",
            "sos index: 2\n",
            "eos index: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHWJroYxWYG0"
      },
      "source": [
        "def batch_proc(data_batch):\r\n",
        "  src_batch = []\r\n",
        "  tar_batch = []\r\n",
        "  \r\n",
        "  for (src_item, tar_item) in data_batch:\r\n",
        "    src_batch.append(torch.cat([torch.tensor([SOS_IDX]), src_item , torch.tensor([EOS_IDX])], dim=0))\r\n",
        "    tar_batch.append(torch.cat([torch.tensor([SOS_IDX]), tar_item , torch.tensor([EOS_IDX])], dim=0))\r\n",
        "  \r\n",
        "  src_batch = pad_sequence(src_batch, padding_value= PAD_IDX)\r\n",
        "  tar_batch = pad_sequence(tar_batch, padding_value= PAD_IDX)\r\n",
        "\r\n",
        "  return src_batch, tar_batch"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5p6tONqX4Zn"
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "train_iter = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn= batch_proc)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G74Wy36CX58J",
        "outputId": "a4717b2f-5a50-47a0-dc73-309e729d64db"
      },
      "source": [
        "a = next(iter(train_iter))\r\n",
        "print(a)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
            "        [17, 15, 10, 15, 23, 24, 17, 10, 23,  7, 15,  4, 10,  6, 22,  4,  4,  9,\n",
            "         21, 11, 19,  4, 21, 17, 18, 25, 11, 14,  7, 15,  9,  5, 15,  6, 23, 21,\n",
            "         21, 25, 11, 18, 17, 23, 18, 14, 15, 21,  4,  7,  4,  4, 14, 11, 17, 24,\n",
            "          5, 21, 14, 17, 21, 11,  6, 14,  7, 16],\n",
            "        [16,  4,  4, 13,  4,  5,  9,  4,  9,  4, 16, 24,  4,  4,  9, 12, 11,  8,\n",
            "          4,  8, 16, 20,  4,  8,  8,  4,  7,  8,  4, 13,  8,  6,  4,  4,  4,  4,\n",
            "         16,  4,  9,  4,  4,  4, 16, 16,  4,  4,  4, 16,  7,  6, 16,  4,  4,  4,\n",
            "         11, 16,  9,  4, 16,  4,  4, 13, 16, 21],\n",
            "        [15,  6, 22,  9,  9,  7, 13, 26,  4,  6,  7,  4, 14,  6, 16, 15,  9,  6,\n",
            "         22, 22,  4,  8,  5,  5,  8, 11, 16, 26, 10,  6, 14, 14, 23,  5, 19,  9,\n",
            "         12, 16,  4,  5, 15,  6, 10,  9,  6, 20, 10, 15,  5,  6, 17, 17, 24, 25,\n",
            "          4, 23,  4, 10, 12,  7, 16, 19, 17,  4],\n",
            "        [17, 13,  9,  5,  5,  4, 13,  8,  6, 19, 14, 28,  5, 14, 14,  6,  4, 14,\n",
            "          9,  9,  3, 19, 14, 11, 10,  8, 12, 13,  5, 11, 12, 13,  9,  6,  4,  4,\n",
            "         10, 14, 11,  6, 21, 19,  9,  4, 11,  4, 11,  4, 11,  5,  9,  9,  5,  4,\n",
            "         18,  4,  7,  9,  4, 28, 18,  4, 13, 14],\n",
            "        [16,  9,  5, 11, 12,  9,  6,  7, 17,  4,  8,  8, 17,  5,  5,  8,  7,  7,\n",
            "         22,  6,  1,  5,  4,  9,  3,  7, 20,  6, 17,  8,  8,  6,  4, 13,  9, 22,\n",
            "          8,  3, 22, 17, 20,  5,  3,  5,  4, 12,  3,  3,  4, 19,  4,  4,  7,  3,\n",
            "          3,  3, 11,  3, 10,  4,  4,  7,  3,  8],\n",
            "        [15,  4,  3,  4,  4,  3,  3,  8,  4,  7,  6,  6,  3,  3,  3,  8,  3,  5,\n",
            "          9, 13,  1,  3,  3,  3,  1, 12,  3,  3,  4, 18, 20,  8, 14,  6,  3,  9,\n",
            "          3,  1,  9,  4, 16,  7,  1,  3,  7,  8,  1,  1,  6,  8, 14, 11,  4,  1,\n",
            "          1,  1,  8,  1, 17,  6, 11, 19,  1, 10],\n",
            "        [ 3,  7,  1,  3, 21,  1,  1,  3,  7,  3,  3,  3,  1,  1,  1, 28,  1, 30,\n",
            "          5, 12,  1,  1,  1,  1,  1,  4,  1,  1,  3,  8,  3, 10,  9,  3,  1,  4,\n",
            "          1,  1,  8,  5, 11,  5,  1,  1,  3,  3,  1,  1,  3,  7, 13,  3, 23,  1,\n",
            "          1,  1,  8,  1,  5,  3,  3,  4,  1,  9],\n",
            "        [ 1,  3,  1,  1,  4,  1,  1,  1,  3,  1,  1,  1,  1,  1,  1,  3,  1,  3,\n",
            "         15, 13,  1,  1,  1,  1,  1,  6,  1,  1,  1, 12,  1,  5,  4,  1,  1,  6,\n",
            "          1,  1,  7,  7,  4,  3,  1,  1,  1,  1,  1,  1,  1,  5,  6,  1,  3,  1,\n",
            "          1,  1,  3,  1,  3,  1,  1, 13,  1,  3],\n",
            "        [ 1,  1,  1,  1,  6,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          3, 19,  1,  1,  1,  1,  1, 14,  1,  1,  1, 13,  1,  4,  7,  1,  1, 16,\n",
            "          1,  1,  3,  8,  7,  1,  1,  1,  1,  1,  1,  1,  1,  3,  3,  1,  1,  1,\n",
            "          1,  1,  1,  1,  1,  1,  1,  6,  1,  1],\n",
            "        [ 1,  1,  1,  1,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1, 20,  1,  1,  1,  1,  1,  3,  1,  1,  1,  3,  1,  3,  3,  1,  1,  6,\n",
            "          1,  1,  1, 20, 10,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1,  1,  1,  1,  1,  1,  3,  1,  1],\n",
            "        [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  3,\n",
            "          1,  1,  1, 12,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "        [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1,  1,  4,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "        [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1,  1,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1]]), tensor([[ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
            "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n",
            "        [14, 12, 29, 12, 27, 18, 39, 11, 53,  5, 12, 33, 11,  7, 32, 33, 33, 24,\n",
            "         25, 26, 22, 41, 25, 14, 17, 38, 26, 28,  5, 12, 24, 40, 12,  7, 27, 25,\n",
            "         25, 38, 44, 17, 14, 37, 17, 23, 12, 25, 41,  5, 33, 33, 23, 19, 14, 18,\n",
            "         40, 25, 45, 14, 25, 26,  7, 28,  5, 48],\n",
            "        [20,  7, 32, 16,  4, 10, 30, 18, 15, 35, 20, 18,  4, 15, 30,  8, 44, 31,\n",
            "         32,  9, 20, 21, 31, 31, 13,  4,  6,  9, 11, 16,  9, 15, 53, 31, 22, 24,\n",
            "         30, 35, 31, 31, 12,  4, 20, 20, 15, 21, 11, 30,  5,  7, 20, 39, 18, 50,\n",
            "         19, 30,  5,  4, 20,  4, 49, 16, 20, 25],\n",
            "        [12, 16, 13, 24, 24,  5,  7,  9, 14, 22,  5,  4, 23, 23, 46,  6,  4, 15,\n",
            "          6, 14, 41,  9, 23, 44, 11, 23,  5, 18, 10,  7, 28, 28, 45,  7, 24, 32,\n",
            "          8, 28, 32,  7,  6, 22, 29, 24, 19,  4,  6, 12,  6,  6, 39, 19, 10,  4,\n",
            "          4, 27, 19, 29,  8,  5, 17, 15, 14, 23],\n",
            "        [14, 24,  3, 10, 10, 24,  3,  5,  4,  5,  6, 37, 10, 13, 13, 12,  5, 28,\n",
            "         52,  6,  3, 22,  4,  3,  3,  5, 30, 35, 14,  6,  8, 16,  4, 16,  3,  4,\n",
            "          6,  3,  5, 14, 25, 13,  3,  4,  5,  8, 44,  4, 23,  7, 46,  3,  5,  3,\n",
            "         17,  4, 13,  3,  4,  6, 19, 22, 16,  9],\n",
            "        [20,  5,  1, 19,  8,  3,  1,  9,  5,  3, 23,  9, 14,  3,  3,  7,  3,  6,\n",
            "         10,  7,  1, 13,  3,  1,  1,  8,  8,  7,  4, 26, 13,  7,  5, 15,  1,  7,\n",
            "         11,  1,  3, 31,  6,  5,  1, 51,  3,  9,  4,  3,  7, 13, 16,  1,  4,  1,\n",
            "          3,  3,  3,  1, 11, 37,  3,  5,  3, 29],\n",
            "        [12,  3,  1,  4, 25,  1,  1,  3,  3,  1,  9, 15,  3,  1,  1, 10,  1,  5,\n",
            "         12, 16,  1,  3,  1,  1,  1, 31, 13,  3,  3,  9,  3,  9,  3,  3,  1, 30,\n",
            "          3,  1,  1,  5, 21, 13,  1,  3,  1,  3,  3,  1,  3, 22, 15,  1, 27,  1,\n",
            "          1,  1,  1,  1,  6,  7,  1, 22,  1,  3],\n",
            "        [ 3,  1,  1,  3,  7,  1,  1,  1,  1,  1,  3,  3,  1,  1,  1, 27,  1, 10,\n",
            "          3,  8,  1,  1,  1,  1,  1, 15,  3,  1,  1, 17,  1, 29,  1,  1,  1, 15,\n",
            "          1,  1,  1,  9, 30,  3,  1,  1,  1,  1,  1,  1,  1,  9,  3,  1,  3,  1,\n",
            "          1,  1,  1,  1, 14,  3,  1,  4,  1,  1],\n",
            "        [ 1,  1,  1,  1,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  3,  1, 14,\n",
            "          1, 35,  1,  1,  1,  1,  1, 28,  1,  1,  1,  9,  1, 10,  1,  1,  1,  3,\n",
            "          1,  1,  1,  8, 26,  1,  1,  1,  1,  1,  1,  1,  1,  5,  1,  1,  1,  1,\n",
            "          1,  1,  1,  1, 13,  1,  1, 15,  1,  1],\n",
            "        [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  6,\n",
            "          1, 27,  1,  1,  1,  1,  1,  3,  1,  1,  1,  8,  1, 21,  1,  1,  1,  1,\n",
            "          1,  1,  1,  4,  5,  1,  1,  1,  1,  1,  1,  1,  1, 13,  1,  1,  1,  1,\n",
            "          1,  1,  1,  1,  3,  1,  1, 18,  1,  1],\n",
            "        [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 11,\n",
            "          1, 13,  1,  1,  1,  1,  1,  1,  1,  1,  1, 16,  1,  4,  1,  1,  1,  1,\n",
            "          1,  1,  1,  3,  6,  1,  1,  1,  1,  1,  1,  1,  1,  3,  1,  1,  1,  1,\n",
            "          1,  1,  1,  1,  1,  1,  1,  3,  1,  1],\n",
            "        [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  3,\n",
            "          1,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  3,  1,  3,  1,  1,  1,  1,\n",
            "          1,  1,  1,  1, 11,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "        [ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1,  1,  1,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9yzUBo5Znd-"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqhUMNx-Zp0y"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 input_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim,\r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 100):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n",
        "        \r\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \r\n",
        "                                                  n_heads, \r\n",
        "                                                  pf_dim,\r\n",
        "                                                  dropout, \r\n",
        "                                                  device) \r\n",
        "                                     for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        \r\n",
        "        batch_size = src.shape[0]\r\n",
        "        src_len = src.shape[1]\r\n",
        "        \r\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        \r\n",
        "        #pos = [batch size, src len]\r\n",
        "        \r\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        for layer in self.layers:\r\n",
        "            src = layer(src, src_mask)\r\n",
        "            \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "            \r\n",
        "        return src"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP6z4p84ZtRE"
      },
      "source": [
        "class EncoderLayer(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 hid_dim, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim,  \r\n",
        "                 dropout, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n",
        "                                                                     pf_dim, \r\n",
        "                                                                     dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        #src_mask = [batch size, 1, 1, src len] \r\n",
        "                \r\n",
        "        #self attention\r\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        #positionwise feedforward\r\n",
        "        _src = self.positionwise_feedforward(src)\r\n",
        "        \r\n",
        "        #dropout, residual and layer norm\r\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        return src"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3EWT8XyZv_Y"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        assert hid_dim % n_heads == 0\r\n",
        "        \r\n",
        "        self.hid_dim = hid_dim\r\n",
        "        self.n_heads = n_heads\r\n",
        "        self.head_dim = hid_dim // n_heads\r\n",
        "        \r\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, query, key, value, mask = None):\r\n",
        "        \r\n",
        "        batch_size = query.shape[0]\r\n",
        "        \r\n",
        "        #query = [batch size, query len, hid dim]\r\n",
        "        #key = [batch size, key len, hid dim]\r\n",
        "        #value = [batch size, value len, hid dim]\r\n",
        "                \r\n",
        "        Q = self.fc_q(query)\r\n",
        "        K = self.fc_k(key)\r\n",
        "        V = self.fc_v(value)\r\n",
        "        \r\n",
        "        #Q = [batch size, query len, hid dim]\r\n",
        "        #K = [batch size, key len, hid dim]\r\n",
        "        #V = [batch size, value len, hid dim]\r\n",
        "                \r\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        \r\n",
        "        #Q = [batch size, n heads, query len, head dim]\r\n",
        "        #K = [batch size, n heads, key len, head dim]\r\n",
        "        #V = [batch size, n heads, value len, head dim]\r\n",
        "                \r\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\r\n",
        "        \r\n",
        "        #energy = [batch size, n heads, query len, key len]\r\n",
        "        \r\n",
        "        if mask is not None:\r\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\r\n",
        "        \r\n",
        "        attention = torch.softmax(energy, dim = -1)\r\n",
        "                \r\n",
        "        #attention = [batch size, n heads, query len, key len]\r\n",
        "                \r\n",
        "        x = torch.matmul(self.dropout(attention), V)\r\n",
        "        \r\n",
        "        #x = [batch size, n heads, query len, head dim]\r\n",
        "        \r\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\r\n",
        "        \r\n",
        "        #x = [batch size, query len, n heads, head dim]\r\n",
        "        \r\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\r\n",
        "        \r\n",
        "        #x = [batch size, query len, hid dim]\r\n",
        "        \r\n",
        "        x = self.fc_o(x)\r\n",
        "        \r\n",
        "        #x = [batch size, query len, hid dim]\r\n",
        "        \r\n",
        "        return x, attention"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JTT-qVyZx2x"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\r\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, hid dim]\r\n",
        "        \r\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, pf dim]\r\n",
        "        \r\n",
        "        x = self.fc_2(x)\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, hid dim]\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqAoeShCZ0AK"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 output_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim, \r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 100):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n",
        "        \r\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \r\n",
        "                                                  n_heads, \r\n",
        "                                                  pf_dim, \r\n",
        "                                                  dropout, \r\n",
        "                                                  device)\r\n",
        "                                     for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "                \r\n",
        "        batch_size = trg.shape[0]\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "                            \r\n",
        "        #pos = [batch size, trg len]\r\n",
        "            \r\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\r\n",
        "                \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        for layer in self.layers:\r\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        output = self.fc_out(trg)\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "            \r\n",
        "        return output, attention"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3rxAja4Z1rE"
      },
      "source": [
        "class DecoderLayer(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 hid_dim, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim, \r\n",
        "                 dropout, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n",
        "                                                                     pf_dim, \r\n",
        "                                                                     dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        \r\n",
        "        #self attention\r\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "            \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "            \r\n",
        "        #encoder attention\r\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "                    \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        #positionwise feedforward\r\n",
        "        _trg = self.positionwise_feedforward(trg)\r\n",
        "        \r\n",
        "        #dropout, residual and layer norm\r\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        return trg, attention"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyM1hdotZ3ei"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 encoder, \r\n",
        "                 decoder, \r\n",
        "                 src_pad_idx, \r\n",
        "                 trg_pad_idx, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.src_pad_idx = src_pad_idx\r\n",
        "        self.trg_pad_idx = trg_pad_idx\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "    def make_src_mask(self, src):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        \r\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "\r\n",
        "        return src_mask\r\n",
        "    \r\n",
        "    def make_trg_mask(self, trg):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        \r\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "        \r\n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\r\n",
        "        \r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\r\n",
        "        \r\n",
        "        #trg_sub_mask = [trg len, trg len]\r\n",
        "            \r\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\r\n",
        "        \r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        \r\n",
        "        return trg_mask\r\n",
        "\r\n",
        "    def forward(self, src, trg):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "                \r\n",
        "        src_mask = self.make_src_mask(src)\r\n",
        "        trg_mask = self.make_trg_mask(trg)\r\n",
        "        \r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        \r\n",
        "        enc_src = self.encoder(src, src_mask)\r\n",
        "        \r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "                \r\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        return output, attention"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TApf_AgdZ4RA"
      },
      "source": [
        "INPUT_DIM = len(hi_en_char_vocab)\r\n",
        "OUTPUT_DIM = len(hi_char_vocab)\r\n",
        "HID_DIM = 256\r\n",
        "ENC_LAYERS = 3\r\n",
        "DEC_LAYERS = 3\r\n",
        "ENC_HEADS = 8\r\n",
        "DEC_HEADS = 8\r\n",
        "ENC_PF_DIM = 512\r\n",
        "DEC_PF_DIM = 512\r\n",
        "ENC_DROPOUT = 0.1\r\n",
        "DEC_DROPOUT = 0.1\r\n",
        "\r\n",
        "enc = Encoder(INPUT_DIM, \r\n",
        "              HID_DIM, \r\n",
        "              ENC_LAYERS, \r\n",
        "              ENC_HEADS, \r\n",
        "              ENC_PF_DIM, \r\n",
        "              ENC_DROPOUT, \r\n",
        "              device)\r\n",
        "\r\n",
        "dec = Decoder(OUTPUT_DIM, \r\n",
        "              HID_DIM, \r\n",
        "              DEC_LAYERS, \r\n",
        "              DEC_HEADS, \r\n",
        "              DEC_PF_DIM, \r\n",
        "              DEC_DROPOUT, \r\n",
        "              device)"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y-1jVtUZ7ii"
      },
      "source": [
        "SRC_PAD_IDX = hi_en_char_vocab.stoi['<pad>']\r\n",
        "TRG_PAD_IDX = hi_char_vocab.stoi['<pad>']\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON1z-B5aZ9Ry",
        "outputId": "edccb7b3-aeb4-480a-c83a-be3b119c7f58"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 4,068,717 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5viw5dTZ_G0"
      },
      "source": [
        "def initialize_weights(m):\r\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\r\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86PbtDmraAY4"
      },
      "source": [
        "model.apply(initialize_weights);"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztG-SSYHaCcX"
      },
      "source": [
        "LEARNING_RATE = 0.0005\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLlRL9S-aEDt"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\r\n",
        "length_loss = nn.MSELoss()"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7hxMI1IaFw-"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        \r\n",
        "        src = batch[0]\r\n",
        "        trg = batch[1]\r\n",
        "        \r\n",
        "        \r\n",
        "        src = src.permute(1,0).to(device)\r\n",
        "        trg = trg.permute(1,0).to(device)\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        output, _ = model(src, trg[:,:-1])\r\n",
        "        \r\n",
        "        #output = [batch size, trg len - 1, output dim]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "            \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "        output = output.contiguous().view(-1, output_dim)\r\n",
        "        trg = trg[:,1:].contiguous().view(-1)\r\n",
        "                \r\n",
        "        #output = [batch size * trg len - 1, output dim]\r\n",
        "        #trg = [batch size * trg len - 1]\r\n",
        "            \r\n",
        "        loss = criterion(output, trg) + length_loss(torch.tensor(len(output)/50), torch.tensor(len(trg)/50))\r\n",
        "\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhxpT9A7aH_Z"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "            src = batch[0]\r\n",
        "            trg = batch[1]\r\n",
        "\r\n",
        "            src = src.permute(1,0).to(device)\r\n",
        "            trg = trg.permute(1,0).to(device)\r\n",
        "\r\n",
        "            output, _ = model(src, trg[:,:-1])\r\n",
        "            \r\n",
        "            #output = [batch size, trg len - 1, output dim]\r\n",
        "            #trg = [batch size, trg len]\r\n",
        "            \r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "            output = output.contiguous().view(-1, output_dim)\r\n",
        "            trg = trg[:,1:].contiguous().view(-1)\r\n",
        "            \r\n",
        "            #output = [batch size * trg len - 1, output dim]\r\n",
        "            #trg = [batch size * trg len - 1]\r\n",
        "            \r\n",
        "            loss = criterion(output, trg)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJS7pYybaJo5"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S4FbMaKTaLbe",
        "outputId": "616db6f2-b259-4825-a669-81ae8c6d1118"
      },
      "source": [
        "N_EPOCHS = 100\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, train_iter, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\r\n",
        "\r\n",
        "    num = 1000\r\n",
        "    word = data['hi-en'][num]\r\n",
        "    expected = data['hi'][num]\r\n",
        "\r\n",
        "    s = time.time()\r\n",
        "    res, attention = transliterate_word(word, hi_en_char_vocab, hi_char_vocab, model, device)\r\n",
        "    e = time.time()\r\n",
        "\r\n",
        "    print(word)\r\n",
        "    print(expected)\r\n",
        "    print(\"\".join(res))\r\n",
        "    print(e-s)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 10s\n",
            "\tTrain Loss: 1.225 | Train PPL:   3.404\n",
            "\t Val. Loss: 0.827 |  Val. PPL:   2.286\n",
            "musibat\n",
            "मुसीबत\n",
            "बुस्ट्बिब्ब्ब्टम्ब्ब्ब्ब्ब्ब्ब्ट्ब्ब्ब्ट्ब्ब्स्ट्ब\n",
            "0.16764044761657715\n",
            "Epoch: 02 | Time: 0m 10s\n",
            "\tTrain Loss: 0.833 | Train PPL:   2.301\n",
            "\t Val. Loss: 0.626 |  Val. PPL:   1.871\n",
            "musibat\n",
            "मुसीबत\n",
            "मस्बिमुस्ब्त्बरमस्स्बिम<eos>\n",
            "0.08085083961486816\n",
            "Epoch: 03 | Time: 0m 10s\n",
            "\tTrain Loss: 0.632 | Train PPL:   1.882\n",
            "\t Val. Loss: 0.472 |  Val. PPL:   1.604\n",
            "musibat\n",
            "मुसीबत\n",
            "मुस्बित्स्बियमब्ट्टिबास्ट<eos>\n",
            "0.0880594253540039\n",
            "Epoch: 04 | Time: 0m 10s\n",
            "\tTrain Loss: 0.528 | Train PPL:   1.696\n",
            "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
            "musibat\n",
            "मुसीबत\n",
            "मुस्बित्ब्बिस्ब्ब्ब्ब्ब्बित्ब्ब्ब्ब्ब्बिस्ब्ब्ब्ब्\n",
            "0.18646931648254395\n",
            "Epoch: 05 | Time: 0m 10s\n",
            "\tTrain Loss: 0.469 | Train PPL:   1.598\n",
            "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
            "musibat\n",
            "मुसीबत\n",
            "मुस्बित्स्बाबाबेंट्स्बाबाबाबाबाबाबाबेंट्स्स्स्स्बा\n",
            "0.16937470436096191\n",
            "Epoch: 06 | Time: 0m 10s\n",
            "\tTrain Loss: 0.434 | Train PPL:   1.544\n",
            "\t Val. Loss: 0.344 |  Val. PPL:   1.411\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसिब्ट्स्टेब्ट्‍स्ट्‍स्‍स्टेबट्ट्‍स्‍स्टेबट्‍स्टे\n",
            "0.1735391616821289\n",
            "Epoch: 07 | Time: 0m 10s\n",
            "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
            "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसिब्टेब्स्टेबट<eos>\n",
            "0.05797553062438965\n",
            "Epoch: 08 | Time: 0m 10s\n",
            "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
            "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसिबत्बेद्ब्तिब्ब्स्ट<eos>\n",
            "0.07590293884277344\n",
            "Epoch: 09 | Time: 0m 10s\n",
            "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
            "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसिब्बतस<eos>\n",
            "0.036252498626708984\n",
            "Epoch: 10 | Time: 0m 10s\n",
            "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
            "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसिब्स्बेट्स्स्स्ट<eos>\n",
            "0.0888512134552002\n",
            "Epoch: 11 | Time: 0m 10s\n",
            "\tTrain Loss: 0.327 | Train PPL:   1.386\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसिबेट्बेस्बेस्बेस्बेडेस्बेस्बेस्बेस्बेस्बेस्बेटि\n",
            "0.17647027969360352\n",
            "Epoch: 12 | Time: 0m 10s\n",
            "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसिबत्स<eos>\n",
            "0.03151726722717285\n",
            "Epoch: 13 | Time: 0m 10s\n",
            "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसिबेट्स<eos>\n",
            "0.04041409492492676\n",
            "Epoch: 14 | Time: 0m 10s\n",
            "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसिबेटेक्स<eos>\n",
            "0.05391836166381836\n",
            "Epoch: 15 | Time: 0m 10s\n",
            "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
            "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसिबेट्स<eos>\n",
            "0.034942626953125\n",
            "Epoch: 16 | Time: 0m 10s\n",
            "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
            "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसिबेट्सबेबेबेबेबेबेबेबेबेबेबेबेबेबेबेबेबेबेबेबेब\n",
            "0.2215116024017334\n",
            "Epoch: 17 | Time: 0m 10s\n",
            "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
            "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसिबेटक्सबटैट<eos>\n",
            "0.05974864959716797\n",
            "Epoch: 18 | Time: 0m 10s\n",
            "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
            "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसीबाट्‍बाबाबाबास्टेबट्ट्ट्‍स<eos>\n",
            "0.13731861114501953\n",
            "Epoch: 19 | Time: 0m 10s\n",
            "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
            "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युसिबेट्सिबेबेबेस<eos>\n",
            "0.06688117980957031\n",
            "Epoch: 20 | Time: 0m 10s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.294\n",
            "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युसिब्टेट्‍स<eos>\n",
            "0.05270266532897949\n",
            "Epoch: 21 | Time: 0m 10s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसीबाद्त<eos>\n",
            "0.035048484802246094\n",
            "Epoch: 22 | Time: 0m 10s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
            "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसिबत्स<eos>\n",
            "0.04124903678894043\n",
            "Epoch: 23 | Time: 0m 10s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युसिबेटम्‍टेस<eos>\n",
            "0.05382037162780762\n",
            "Epoch: 24 | Time: 0m 10s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
            "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसीबत<eos>\n",
            "0.025004148483276367\n",
            "Epoch: 25 | Time: 0m 10s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युसिबेटिस<eos>\n",
            "0.04154205322265625\n",
            "Epoch: 26 | Time: 0m 10s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसिबतिब्सिबतिबतिबतिबतिबतिबतिबतिबतिबतिबतिबतिबतिबति\n",
            "0.17498230934143066\n",
            "Epoch: 27 | Time: 0m 10s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
            "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसीबत<eos>\n",
            "0.026412248611450195\n",
            "Epoch: 28 | Time: 0m 10s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युसिबेट्सिस्स्स्स्स्सिम<eos>\n",
            "0.08724451065063477\n",
            "Epoch: 29 | Time: 0m 10s\n",
            "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
            "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युसिबटेटेटेटेट्समटेटेटेटेटेटेटेटेटेटेटेटेटेटेटेट\n",
            "0.1678328514099121\n",
            "Epoch: 30 | Time: 0m 10s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
            "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युज़िबाटक्सीबटेबटम<eos>\n",
            "0.06925654411315918\n",
            "Epoch: 31 | Time: 0m 10s\n",
            "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
            "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युज़िब्बेटेबॉट्सीबॉटेबॉटेबॉटेबॉटेबॉटेबॉटेबॉटेबॉटे\n",
            "0.1906750202178955\n",
            "Epoch: 32 | Time: 0m 10s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युज़िबट्सबट्सबॉसबॉसबॉसबॉसबॉसबॉसबॉसबॉसबॉसबॉसबॉसबॉस\n",
            "0.22272086143493652\n",
            "Epoch: 33 | Time: 0m 10s\n",
            "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
            "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युज़िबेट्स्ट्स्सीब्ट्स्ट्सीबट्ट्सीबट्ट्ट्सीबट्ट्स\n",
            "0.1987137794494629\n",
            "Epoch: 34 | Time: 0m 10s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युसीबटेटेस्टेसिटेस्टेसिटेसिटेस्टेसिटेस्‍स्टेस्‍स\n",
            "0.18941092491149902\n",
            "Epoch: 35 | Time: 0m 10s\n",
            "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
            "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युज़िबेटेस्टेटेटेटेटेटेटेटेटेटेटेटेटेटेटेटेटेटेटे\n",
            "0.1714918613433838\n",
            "Epoch: 36 | Time: 0m 10s\n",
            "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
            "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसीबत्सी<eos>\n",
            "0.045679569244384766\n",
            "Epoch: 37 | Time: 0m 10s\n",
            "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
            "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसीबत<eos>\n",
            "0.026716947555541992\n",
            "Epoch: 38 | Time: 0m 10s\n",
            "\tTrain Loss: 0.174 | Train PPL:   1.189\n",
            "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसीबत्सीबाद<eos>\n",
            "0.050902366638183594\n",
            "Epoch: 39 | Time: 0m 10s\n",
            "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
            "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युसिबेटेटेसीबेटेटेटेटेटेटेटम<eos>\n",
            "0.10460543632507324\n",
            "Epoch: 40 | Time: 0m 11s\n",
            "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
            "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युज़िब्टेसिब्ट्सिब्सीब्सीब्सीब्सीब्सीब्सीब्सीब्सी\n",
            "0.17726659774780273\n",
            "Epoch: 41 | Time: 0m 10s\n",
            "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
            "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युज़िबॅटक्स<eos>\n",
            "0.04510951042175293\n",
            "Epoch: 42 | Time: 0m 10s\n",
            "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
            "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसीबत्दी<eos>\n",
            "0.03645586967468262\n",
            "Epoch: 43 | Time: 0m 10s\n",
            "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
            "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युज़िबेट्‍िसीबसीबसीबसीबटक्सीबसिबसीबसीबसिटक्सीबसीब\n",
            "0.1981039047241211\n",
            "Epoch: 44 | Time: 0m 10s\n",
            "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
            "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युज़िबेट्‍सक्स<eos>\n",
            "0.0543065071105957\n",
            "Epoch: 45 | Time: 0m 10s\n",
            "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
            "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसीबत्स<eos>\n",
            "0.0316929817199707\n",
            "Epoch: 46 | Time: 0m 10s\n",
            "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युज़िबेट्‍सेटेटेबेटेबेटेटेट्स<eos>\n",
            "0.10588979721069336\n",
            "Epoch: 47 | Time: 0m 10s\n",
            "\tTrain Loss: 0.151 | Train PPL:   1.164\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "musibat\n",
            "मुसीबत\n",
            "मुसीबत्सीबत<eos>\n",
            "0.04192066192626953\n",
            "Epoch: 48 | Time: 0m 9s\n",
            "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युज़िबेट्‍बेक्‍ट्‍यम<eos>\n",
            "0.07535338401794434\n",
            "Epoch: 49 | Time: 0m 9s\n",
            "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "musibat\n",
            "मुसीबत\n",
            "म्युज़िबाट<eos>\n",
            "0.03557419776916504\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-147-770b3473eae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-144-51b7f0e5d122>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    group['eps'])\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV4zptw-bFSg"
      },
      "source": [
        "def transliterate_word (word, src_vocab, trg_vocab, model, device, max_len = 50):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "        \r\n",
        "    if isinstance(word, str):\r\n",
        "        tokens = tokenize(word)\r\n",
        "    else:\r\n",
        "        tokens = [token.lower() for token in word]\r\n",
        "\r\n",
        "    tokens = [src_vocab['<sos>']] + tokens + [src_vocab['<eos>']]\r\n",
        "        \r\n",
        "    src_indexes = [src_vocab.stoi[token] for token in tokens]\r\n",
        "\r\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\r\n",
        "    \r\n",
        "    src_mask = model.make_src_mask(src_tensor)\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\r\n",
        "\r\n",
        "    trg_indexes = [trg_vocab.stoi['<sos>']]\r\n",
        "\r\n",
        "    for i in range(max_len):\r\n",
        "\r\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\r\n",
        "\r\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\r\n",
        "        \r\n",
        "        with torch.no_grad():\r\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        pred_token = output.argmax(2)[:,-1].item()\r\n",
        "        \r\n",
        "        trg_indexes.append(pred_token)\r\n",
        "\r\n",
        "        if pred_token == trg_vocab.stoi['<eos>']:\r\n",
        "            break\r\n",
        "\r\n",
        "    trg_tokens = [trg_vocab.itos[i] for i in trg_indexes]\r\n",
        "    \r\n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW4LR9dJtAB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8cce032-114c-4e12-a900-03951aefb3b2"
      },
      "source": [
        "for  num in range(10):\r\n",
        "  # num = 10\r\n",
        "  word = data['hi-en'][num]\r\n",
        "  expected = data['hi'][num]\r\n",
        "\r\n",
        "  s = time.time()\r\n",
        "  res, attention = transliterate_word(word, hi_en_char_vocab, hi_char_vocab, model, device)\r\n",
        "  e = time.time()\r\n",
        "\r\n",
        "  print(word)\r\n",
        "  print(expected)\r\n",
        "  print(\"\".join(res))\r\n",
        "  print(e-s)\r\n",
        "  print('\\n')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "suna\n",
            "सुना\n",
            "सुनअद्दुनादुल‍िनादुनादुनादुनादुनादुनाद्‍िक्‍सुनादु\n",
            "0.21206927299499512\n",
            "\n",
            "\n",
            "tha\n",
            "था\n",
            "थाड़ी<eos>\n",
            "0.01846909523010254\n",
            "\n",
            "\n",
            "dekha\n",
            "देखा\n",
            "देखड़ेखड़ेखड़ी<eos>\n",
            "0.04222536087036133\n",
            "\n",
            "\n",
            "na\n",
            "न\n",
            "नादप्‍दीनादाप्‍जन‍दन‍दन‍दन‍दन‍दन‍दन‍दन‍दन‍दन‍दाप्‍\n",
            "0.17318964004516602\n",
            "\n",
            "\n",
            "samajha\n",
            "समझा\n",
            "समझाअब्‍िसादाल्ता<eos>\n",
            "0.06345057487487793\n",
            "\n",
            "\n",
            "jana\n",
            "जाना\n",
            "जनाद्दनादीपीदादादनादादादादादादादादादादादादादादादना\n",
            "0.1667799949645996\n",
            "\n",
            "\n",
            "han\n",
            "हां\n",
            "हंचप्धंढपु<eos>\n",
            "0.0372006893157959\n",
            "\n",
            "\n",
            "kya\n",
            "क्या\n",
            "क्याद्दधधधधधधड़क्देश्दक्‍दक्‍ियादक्‍दक्‍दक्‍दक्‍दक्\n",
            "0.17278361320495605\n",
            "\n",
            "\n",
            "yahi\n",
            "यही\n",
            "यहिद्धियाधि<eos>\n",
            "0.045709848403930664\n",
            "\n",
            "\n",
            "vo\n",
            "वो\n",
            "वोवोडव्सवास्डवाबीवाल्डवोडवाज़ेस<eos>\n",
            "0.10198736190795898\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLF4Hl0KyLIl"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\r\n",
        "\r\n",
        "def calculate_bleu(data, src_vocab, trg_vocab, model, device, max_len = 50):\r\n",
        "    \r\n",
        "    trgs = []\r\n",
        "    pred_trgs = []\r\n",
        "\r\n",
        "    src_data = data['hi-en']\r\n",
        "    trg_data = data['hi']\r\n",
        "    \r\n",
        "    for src,trg in zip(src_data, trg_data):\r\n",
        "        \r\n",
        "        pred_trg, _ = transliterate_word(src, src_vocab, trg_vocab, model, device, max_len)\r\n",
        "        \r\n",
        "        #cut off <eos> token\r\n",
        "        pred_trg = pred_trg[:-1]\r\n",
        "        \r\n",
        "        pred_trgs.append(pred_trg)\r\n",
        "        trgs.append([trg])\r\n",
        "        \r\n",
        "    return bleu_score(pred_trgs, trgs)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsxEkRd3yoDi"
      },
      "source": [
        "bleu_score = calculate_bleu(data, hi_en_char_vocab, hi_char_vocab, model, device)\r\n",
        "\r\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UQcpcTEyu3S"
      },
      "source": [
        "def translate_sentence_vectorized(src_tensor, src_vocab, trg_vocab, model, device, max_len=50):\r\n",
        "    assert isinstance(src_tensor, torch.Tensor)\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "    src_mask = model.make_src_mask(src_tensor)\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\r\n",
        "    # enc_src = [batch_sz, src_len, hid_dim]\r\n",
        "\r\n",
        "    trg_indexes = [[trg_vocab.stoi['<sos>']] for _ in range(len(src_tensor))]\r\n",
        "    # Even though some examples might have been completed by producing a <eos> token\r\n",
        "    # we still need to feed them through the model because other are not yet finished\r\n",
        "    # and all examples act as a batch. Once every single sentence prediction encounters\r\n",
        "    # <eos> token, then we can stop predicting.\r\n",
        "    translations_done = [0] * len(src_tensor)\r\n",
        "    for i in range(max_len):\r\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).to(device)\r\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\r\n",
        "        with torch.no_grad():\r\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\r\n",
        "        pred_tokens = output.argmax(2)[:,-1]\r\n",
        "        for i, pred_token_i in enumerate(pred_tokens):\r\n",
        "            trg_indexes[i].append(pred_token_i)\r\n",
        "            if pred_token_i == trg_vocab.stoi['<eos>']:\r\n",
        "                translations_done[i] = 1\r\n",
        "        if all(translations_done):\r\n",
        "            break\r\n",
        "\r\n",
        "    # Iterate through each predicted example one by one;\r\n",
        "    # Cut-off the portion including the after the <eos> token\r\n",
        "    pred_sentences = []\r\n",
        "    for trg_sentence in trg_indexes:\r\n",
        "        pred_sentence = []\r\n",
        "        for i in range(1, len(trg_sentence)):\r\n",
        "            if trg_sentence[i] == trg_vocab.stoi['<eos>']:\r\n",
        "                break\r\n",
        "            pred_sentence.append(trg_vocab.itos[trg_sentence[i]])\r\n",
        "        pred_sentences.append(pred_sentence)\r\n",
        "\r\n",
        "    return pred_sentences, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt1InOX2zFaC"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\r\n",
        "\r\n",
        "def calculate_bleu_alt(iterator, src_vocab, trg_vocab, model, device, max_len = 50):\r\n",
        "    trgs = []\r\n",
        "    pred_trgs = []\r\n",
        "    with torch.no_grad():\r\n",
        "        for batch in iterator:\r\n",
        "            src = batch[0]\r\n",
        "            trg = batch[1]\r\n",
        "\r\n",
        "            src = src.permute(1,0).to(device)\r\n",
        "            trg = trg.permute(1,0).to(device)\r\n",
        "\r\n",
        "            _trgs = []\r\n",
        "            for sentence in trg:\r\n",
        "                tmp = []\r\n",
        "                # Start from the first token which skips the <start> token\r\n",
        "                for i in sentence[1:]:\r\n",
        "                    # Targets are padded. So stop appending as soon as a padding or eos token is encountered\r\n",
        "                    if i == trg_vocab.stoi['<eos>'] or i == trg_vocab.stoi['<pad>']:\r\n",
        "                        break\r\n",
        "                    tmp.append(trg_vocab.itos[i])\r\n",
        "                _trgs.append([tmp])\r\n",
        "            trgs += _trgs\r\n",
        "            pred_trg, _ = translate_sentence_vectorized(src, src_vocab, trg_vocab, model, device)\r\n",
        "            pred_trgs += pred_trg\r\n",
        "    return bleu_score(pred_trgs, trgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0ecGCgn_XqC"
      },
      "source": [
        "bleu_score = calculate_bleu_alt(train_iter, hi_en_char_vocab, hi_char_vocab, model, device)\r\n",
        "\r\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm7mODIv_fKi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}