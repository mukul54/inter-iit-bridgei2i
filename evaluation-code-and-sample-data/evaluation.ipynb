{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtQ_dZW0gfdD"
   },
   "source": [
    "# Evaluate Theme Clasification and Summarization\n",
    "\n",
    "1. **Theme classification**: Precision, Recall and F1 Score \n",
    "2. **Entity based sentiment**: Accuracy of Brand identification and Precision, Recall and F1 Score Sentiment \n",
    "3. **Automated Headlines evaluation**: (*Note: The generated Headlines need to be in English irrespective of the language in the article*)\n",
    "Average similarity scores of AI generated headlines compared with actual headlines would be used as a metric for evaluation.\n",
    "4. **Rough and BLEU score**: To evaluate the language summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1121,
     "status": "ok",
     "timestamp": 1614701192644,
     "user": {
      "displayName": "Akash Rawat",
      "photoUrl": "",
      "userId": "05064438118558107156"
     },
     "user_tz": -330
    },
    "id": "bEiMbOVsPSDX",
    "outputId": "6e8a570c-dfd9-4aa6-d4d6-d531dee3462d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1117,
     "status": "ok",
     "timestamp": 1614701192648,
     "user": {
      "displayName": "Akash Rawat",
      "photoUrl": "",
      "userId": "05064438118558107156"
     },
     "user_tz": -330
    },
    "id": "UZpfuQHhnfCL",
    "outputId": "148d83dd-4ad6-409e-91da-d9dba5e13521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your working directory was changed to /content/drive/My Drive/Colab Notebooks/Text Summarization and classification/\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# the base Google Drive directory\n",
    "root_dir = \"/content/drive/My Drive/\"\n",
    "\n",
    "# choose where you want your project files to be saved\n",
    "project_folder = \"Colab Notebooks/Text Summarization and classification/\"\n",
    "\n",
    "def create_and_set_working_directory(project_folder):\n",
    "  # check if your project folder exists. if not, it will be created.\n",
    "  if os.path.isdir(root_dir + project_folder) == False:\n",
    "    os.mkdir(root_dir + project_folder)\n",
    "    print(root_dir + project_folder + ' did not exist but was created.')\n",
    "\n",
    "  # change the OS to use your project folder as the working directory\n",
    "  os.chdir(root_dir + project_folder)\n",
    "\n",
    "  # create a test file to make sure it shows up in the right place\n",
    "  #!touch 'new_file_in_working_directory.txt'\n",
    "  print('\\nYour working directory was changed to ' + root_dir + project_folder)\n",
    "\n",
    "create_and_set_working_directory(project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9789,
     "status": "ok",
     "timestamp": 1614701201326,
     "user": {
      "displayName": "Akash Rawat",
      "photoUrl": "",
      "userId": "05064438118558107156"
     },
     "user_tz": -330
    },
    "id": "q5LeVq589mJk",
    "outputId": "5eb72b71-7440-4173-d0a1-3708cff9b0bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: sentence-transformers in /usr/local/lib/python3.7/dist-packages (0.4.1.2)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.1+cu101)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.95)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.3.3)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.7.0)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.1)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
      "Requirement already up-to-date: rouge in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
      "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.7/dist-packages (0.18.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers\n",
    "!pip install -U rouge\n",
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9785,
     "status": "ok",
     "timestamp": 1614701201327,
     "user": {
      "displayName": "Akash Rawat",
      "photoUrl": "",
      "userId": "05064438118558107156"
     },
     "user_tz": -330
    },
    "id": "JPIrUh0pZQTz",
    "outputId": "32cb37e4-9cc1-434f-e48d-e0f4b132b6ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from rouge import Rouge\n",
    "from statistics import mean\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from fuzzywuzzy import fuzz\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6JroNatp8FK"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XePPoI0pO6nH"
   },
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "output1_df = pd.read_csv(\"Output Data/sample_output_1.csv\")\n",
    "output2_df = pd.read_csv(\"Output Data/sample_output_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTB3abIj0uWc"
   },
   "outputs": [],
   "source": [
    "# Data subsetting\n",
    "\n",
    "# Articles\n",
    "count_article_df = output1_df[output1_df['Text_ID'].astype(str).str.contains('article', case=False, regex=True)]\n",
    "df1 = count_article_df['Mobile_Tech_Flag_Actual'].value_counts()\n",
    "article_mobile_tech, article_others = df1[1], df1[0]\n",
    "\n",
    "# Tweets\n",
    "count_tweet_df = output1_df[output1_df['Text_ID'].astype(str).str.contains('tweet', case=False, regex=True)]\n",
    "df2 = count_tweet_df['Mobile_Tech_Flag_Actual'].value_counts()\n",
    "tweet_mobile_tech, tweet_others = df2[1], df2[0]\n",
    "\n",
    "# Articles and Tweets with mobile tech\n",
    "mobile_tech_count_df = pd.DataFrame(columns=['Mobile_Tech', 'Others'])\n",
    "mobile_tech_count_df.loc['Articles',:] = [article_mobile_tech,article_others]\n",
    "mobile_tech_count_df.loc['Tweets',:] = [tweet_mobile_tech,tweet_others]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4-SS-IRqBA9"
   },
   "source": [
    "## Theme Classification Metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKOcHCIxVJG_"
   },
   "outputs": [],
   "source": [
    "# Overall\n",
    "# Theme classification Actual and Predicted labels\n",
    "y_mobiletech_true = output1_df['Mobile_Tech_Flag_Actual'].values\n",
    "y_mobiletech_pred = output1_df['Mobile_Tech_Flag_Predicted'].values\n",
    "\n",
    "# Get scores for Theme classification\n",
    "precision = round(precision_score(y_mobiletech_true, y_mobiletech_pred),2)\n",
    "recall = round(recall_score(y_mobiletech_true, y_mobiletech_pred),2)\n",
    "f_score = round(f1_score(y_mobiletech_true, y_mobiletech_pred),2)\n",
    "acc_score = round(accuracy_score(y_mobiletech_true, y_mobiletech_pred),2)\n",
    "\n",
    "# Article\n",
    "# Theme classification Actual and Predicted labels\n",
    "article1_df = output1_df[output1_df['Text_ID'].astype(str).str.contains('article', case=False, regex=True)]\n",
    "article1_mobiletech_true = article1_df['Mobile_Tech_Flag_Actual'].values\n",
    "article1_mobiletech_pred = article1_df['Mobile_Tech_Flag_Predicted'].values\n",
    "\n",
    "# Get scores for Theme classification\n",
    "article_precision = round(precision_score(article1_mobiletech_true, article1_mobiletech_pred),2)\n",
    "article_recall = round(recall_score(article1_mobiletech_true, article1_mobiletech_pred),2)\n",
    "article_f_score = round(f1_score(article1_mobiletech_true, article1_mobiletech_pred),2)\n",
    "article_acc_score = round(accuracy_score(article1_mobiletech_true, article1_mobiletech_pred),2)\n",
    "\n",
    "# Tweets\n",
    "# Theme classification Actual and Predicted labels\n",
    "tweet1_df = output1_df[output1_df['Text_ID'].astype(str).str.contains('tweet', case=False, regex=True)]\n",
    "tweet_mobiletech_true = tweet1_df['Mobile_Tech_Flag_Actual'].values\n",
    "tweet_mobiletech_pred = tweet1_df['Mobile_Tech_Flag_Predicted'].values\n",
    "\n",
    "# Get scores for Theme classification\n",
    "tweet_precision = round(precision_score(tweet_mobiletech_true, tweet_mobiletech_pred),2)\n",
    "tweet_recall = round(recall_score(tweet_mobiletech_true, tweet_mobiletech_pred),2)\n",
    "tweet_f_score = round(f1_score(tweet_mobiletech_true, tweet_mobiletech_pred),2)\n",
    "tweet_acc_score = round(accuracy_score(tweet_mobiletech_true, tweet_mobiletech_pred),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FT9ksS0SpkOZ"
   },
   "source": [
    "## Calculate Headline Similarity\n",
    "  - Actual Headline\n",
    "  - Generated Headline in Original Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCQA88JpZUli"
   },
   "outputs": [],
   "source": [
    "# Headlines Actual and Predicted\n",
    "headline_df = output1_df[~output1_df['Headline_Actual_Eng'].isna()]\n",
    "y_headlines_true = headline_df['Headline_Actual_Eng']\n",
    "y_headlines_pred = headline_df['Headline_Generated_Eng_Lang']\n",
    "\n",
    "# Get average document similarity using BERT\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "headline_similarity = []\n",
    "for actual_headline, predicted_headline in zip(y_headlines_true, y_headlines_pred):\n",
    "  actual_headline_embeddings = model.encode(actual_headline) # Get a vector for each headlines\n",
    "  predicted_headline_embeddings = model.encode(predicted_headline) # Get a vector for each headlines\n",
    "  distance = scipy.spatial.distance.cdist([actual_headline_embeddings], [predicted_headline_embeddings], \"cosine\")[0]\n",
    "  headline_similarity.append(\"%.4f\" % (1-distance))\n",
    "\n",
    "headline_similarity = list(map(float, headline_similarity))\n",
    "avg_headline_sim_score = round(mean(headline_similarity),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qq-WZzMHpP3r"
   },
   "source": [
    "## Calculate Rouge Scores for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8md6PVMIiITh"
   },
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "\n",
    "# Headlines Actual and Predicted\n",
    "headline_df = output1_df[~output1_df['Headline_Actual_Eng'].isna()]\n",
    "y_headlines_true = headline_df['Headline_Actual_Eng']\n",
    "y_headlines_pred = headline_df['Headline_Generated_Eng_Lang']\n",
    "\n",
    "rouge_scores = []\n",
    "for actual_headline, predicted_headline in zip(y_headlines_true, y_headlines_pred):\n",
    "  rouge_score = rouge.get_scores(actual_headline, predicted_headline)\n",
    "  rouge_scores.append(rouge_score[0]['rouge-l'])\n",
    "\n",
    "# Averaging the scores\n",
    "f = [score['f'] for score in rouge_scores]\n",
    "p = [score['p'] for score in rouge_scores]\n",
    "r = [score['r'] for score in rouge_scores]\n",
    "\n",
    "avg_rogue_scores = {'F1 Score':round(mean(f),2),\n",
    "                    'Precision':round(mean(p),2),\n",
    "                    'Recall':round(mean(r),2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwLDBYcnpJUM"
   },
   "source": [
    "## Calculate BLEU Scores for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGIKNc99iTH_"
   },
   "outputs": [],
   "source": [
    "headline_df = output1_df[~output1_df['Headline_Actual_Eng'].isna()]\n",
    "\n",
    "org_headlines = headline_df['Headline_Actual_Eng'] # summarized texts\n",
    "gen_headlines = headline_df['Headline_Generated_Eng_Lang'] # summarized texts\n",
    "\n",
    "bleu_scores = []\n",
    "for org_headline, gen_headline in zip(org_headlines, gen_headlines):\n",
    "  hypothesis = gen_headline.split()\n",
    "  reference = org_headline.split()\n",
    "  references = [reference] # list of references for 1 sentence.\n",
    "  list_of_references = [references] # list of references for all sentences in corpus.\n",
    "  list_of_hypotheses = [hypothesis] # list of hypotheses that corresponds to list of references.\n",
    "  bleu_score = corpus_bleu(list_of_references, list_of_hypotheses)\n",
    "  bleu_scores.append(bleu_score)\n",
    "\n",
    "avg_bleu_scores = round(mean(bleu_scores),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugjAPrlSpXZR"
   },
   "source": [
    "## Evaluate Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBzmsUhJeG9p"
   },
   "outputs": [],
   "source": [
    "def map_sentiments(sentiment):\n",
    "  '''\n",
    "  Map sentiment tags to labels\n",
    "    - negative : 0\n",
    "    - positive : 1      \n",
    "    - neutral : 2\n",
    "  '''\n",
    "  sentiment = sentiment.lower()\n",
    "  if sentiment == 'positive':\n",
    "    return 1\n",
    "  elif sentiment == 'negative':\n",
    "    return 0\n",
    "  else:\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rf93hFGo9bu_"
   },
   "source": [
    "# Brand Similarity and Sentiment Score Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIWHJ2zCVs-a"
   },
   "outputs": [],
   "source": [
    "# fuzzy matching for Brands\n",
    "sentiment_df = output2_df[~(output2_df['Brands_Entity_Actual'].isna() & output2_df['Brands_Entity_Identified'].isna())]\n",
    "\n",
    "def compute_sim(text, my_list, threshold):\n",
    "  #score_list = [(fuzz.ratio(text, x), x) for x in my_list]\n",
    "  score_list = [(1, x) for x in my_list if fuzz.ratio(text, x)>threshold]\n",
    "  # print(score_list)\n",
    "  if score_list:\n",
    "    return 1, str(score_list[0][1])\n",
    "  else:\n",
    "    return 0, \"\"\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "for id in sentiment_df['Text_ID'].unique():\n",
    "  temp_df = output2_df.loc[output2_df['Text_ID'] == id, ].reset_index(drop=True)\n",
    "\n",
    "  actual_list, predict_list = list(temp_df['Brands_Entity_Actual'].unique()), list(temp_df['Brands_Entity_Identified'].unique())\n",
    "\n",
    "  temp_df_actual = temp_df[['Text_ID','Brands_Entity_Actual','Sentiment_Actual']].drop_duplicates()\n",
    "  temp_df_predict = temp_df[['Text_ID','Brands_Entity_Identified','Sentiment_Identified']].drop_duplicates(subset=['Text_ID','Brands_Entity_Identified'])\n",
    "\n",
    "  temp_df_actual = temp_df_actual.reset_index(drop=True)\n",
    "  temp_df_predict = temp_df_predict.reset_index(drop=True)\n",
    "\n",
    "  temp_df_actual['Actual_Brand'], temp_df_predict['Prediction_Brand'] = 0,0\n",
    "  temp_df_actual['Actual_Sent'], temp_df_predict['Prediction_Sent'] = 0,0\n",
    "\n",
    "  for i in range(temp_df_actual.shape[0]):\n",
    "    temp_df_actual.loc[i,'Actual_Brand'], match_text = compute_sim(temp_df_actual.loc[i, 'Brands_Entity_Actual'], my_list=predict_list, threshold=0.7)\n",
    "    if temp_df_actual.loc[i,'Actual_Brand'] == 1:\n",
    "      df_A = temp_df_predict.loc[temp_df_predict['Brands_Entity_Identified'] == match_text,'Sentiment_Identified'].reset_index(drop=True)\n",
    "      temp_df_actual.loc[i,'Actual_Sent'] = bool(temp_df_actual.loc[i,'Sentiment_Actual'] == df_A[0])\n",
    "\n",
    "  for i in range(temp_df_predict.shape[0]):\n",
    "    temp_df_predict.loc[i,'Prediction_Brand'], match_text = compute_sim(str(temp_df_predict.loc[i, 'Brands_Entity_Identified']), actual_list, threshold=0.7)\n",
    "    if temp_df_predict.loc[i,'Prediction_Brand'] == 1:\n",
    "      df_B = temp_df_actual.loc[temp_df_actual['Brands_Entity_Actual'] == match_text,'Sentiment_Actual'].reset_index(drop=True)\n",
    "      temp_df_predict.loc[i,'Prediction_Sent'] = bool(temp_df_predict.loc[i,'Sentiment_Identified'] == df_B[0])\n",
    "\n",
    "  temp_df1 = pd.DataFrame({'Text_ID': id,\n",
    "                           'Recall_Brand': round(temp_df_actual['Actual_Brand'].sum()/(len(temp_df_actual)),2),\n",
    "                           'Precision_Brand': round(temp_df_predict['Prediction_Brand'].sum()/len(temp_df_predict),2),\n",
    "                           'Recall_Sent': round(temp_df_actual['Actual_Sent'].sum()/(len(temp_df_actual)),2),\n",
    "                           'Precision_Sent': round(temp_df_predict['Prediction_Sent'].sum()/len(temp_df_predict),2)}, index=[0])\n",
    "\n",
    "  final_df = final_df.append(temp_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgF5TsfNWjIP"
   },
   "outputs": [],
   "source": [
    "# Text Sentiment Scores\n",
    "# Overall\n",
    "overall_sent_precision = round(final_df['Precision_Sent'].mean(),2)\n",
    "overall_sent_recall = round(final_df['Recall_Sent'].mean(),2)\n",
    "\n",
    "# article\n",
    "article2_df = final_df[final_df['Text_ID'].astype(str).str.contains('article', case=False, regex=True)]\n",
    "article_sent_precision = round(article2_df['Precision_Sent'].mean(),2)\n",
    "article_sent_recall = round(article2_df['Recall_Sent'].mean(),2)\n",
    "\n",
    "# tweets\n",
    "tweet2_df = final_df[final_df['Text_ID'].astype(str).str.contains('tweet', case=False, regex=True)]\n",
    "tweet_sent_precision = round(tweet2_df['Precision_Sent'].mean(),2)\n",
    "tweet_sent_recall = round(tweet2_df['Recall_Sent'].mean(),2)\n",
    "\n",
    "# Brand Prediction Scores\n",
    "# Overall\n",
    "overall_brand_precision = round(final_df['Precision_Brand'].mean(),2)\n",
    "overall_brand_recall = round(final_df['Recall_Brand'].mean(),2)\n",
    "\n",
    "# article\n",
    "article2_df = final_df[final_df['Text_ID'].astype(str).str.contains('article', case=False, regex=True)]\n",
    "article_brand_precision = round(article2_df['Precision_Brand'].mean(),2)\n",
    "article_brand_recall = round(article2_df['Recall_Brand'].mean(),2)\n",
    "\n",
    "# tweets\n",
    "tweet2_df = final_df[final_df['Text_ID'].astype(str).str.contains('tweet', case=False, regex=True)]\n",
    "tweet_brand_precision = round(tweet2_df['Precision_Brand'].mean(),2)\n",
    "tweet_brand_recall = round(tweet2_df['Recall_Brand'].mean(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t4xmNrKpcVB"
   },
   "source": [
    "## Generate Evaluation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14729,
     "status": "ok",
     "timestamp": 1614701206311,
     "user": {
      "displayName": "Akash Rawat",
      "photoUrl": "",
      "userId": "05064438118558107156"
     },
     "user_tz": -330
    },
    "id": "UmZWtZrWKnpG",
    "outputId": "eab43e9c-5596-47fa-e790-4f3794b36401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**  Data Overview **\n",
      "         Mobile_Tech Others\n",
      "Articles           8      7\n",
      "Tweets            14      1\n",
      "============================== Article ==============================\n",
      "1. Theme classification evaluation (for mobile tech):\n",
      "> Precision : 57.0%\n",
      "> Recall : 50.0%\n",
      "> F1 Score: 53.0%\n",
      "> Accuracy Score: 60.0%\n",
      "-----\n",
      "2. Entity based evaluation\n",
      "Brand Identification:\n",
      "> Precision : 67.0%\n",
      "> Recall : 100.0%\n",
      "-----\n",
      "Sentiment Analysis:\n",
      "> Precision : 33.0%\n",
      "> Recall : 0.0%\n",
      "-----\n",
      "3. Automated Headlines evaluation\n",
      "--> Average similarity scores: 66.0%\n",
      "--> Rough Score:\n",
      "\t> F1 Score : 33.0%\n",
      "\t> Precision : 29.0%\n",
      "\t> Recall : 42.0%\n",
      "--> BLEU Score: 32.0%\n",
      "----------------------------------------------------------------------\n",
      "============================== Tweet ==============================\n",
      "1. Theme classification evaluation (for mobile tech):\n",
      "> Precision : 100.0%\n",
      "> Recall : 64.0%\n",
      "> F1 Score: 78.0%\n",
      "-----\n",
      "2. Entity based evaluation\n",
      "Brand Identification:\n",
      "> Precision : 100.0%\n",
      "> Recall : 100.0%\n",
      "-----\n",
      "Sentiment Analysis:\n",
      "> Precision : 75.0%\n",
      "> Recall : 75.0%\n",
      "----------------------------------------------------------------------\n",
      "============================== Overall ==============================\n",
      "1. Theme classification evaluation (for mobile tech):\n",
      "> Precision : 81.0%\n",
      "> Recall : 59.0%\n",
      "> F1 Score: 68.0%\n",
      "> Accuracy Score: 60.0%\n",
      "-----\n",
      "2. Entity based evaluation\n",
      "Brand Identification:\n",
      "> Precision : 97.0%\n",
      "> Recall : 100.0%\n",
      "-----\n",
      "Sentiment Analysis:\n",
      "> Precision : 71.0%\n",
      "> Recall : 68.0%\n",
      "3. Automated Headlines evaluation\n",
      "--> Average similarity scores: 66.0%\n",
      "--> Rough Score:\n",
      "\t> F1 Score : 33.0%\n",
      "\t> Precision : 29.0%\n",
      "\t> Recall : 42.0%\n",
      "--> BLEU Score : 32.0%\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"**  Data Overview **\")\n",
    "print(mobile_tech_count_df)\n",
    "\n",
    "print(\"=\"*30, \"Article\", \"=\"*30)\n",
    "print(\"1. Theme classification evaluation (for mobile tech):\")\n",
    "print('> Precision : {}%'.format(round(article_precision*100,2)))\n",
    "print(f'> Recall : {article_recall*100}%')\n",
    "print(f'> F1 Score: {article_f_score*100}%')\n",
    "print(f'> Accuracy Score: {acc_score*100}%')\n",
    "print('-'*5)\n",
    "print(\"2. Entity based evaluation\")\n",
    "print(\"Brand Identification:\")\n",
    "print(f'> Precision : {article_brand_precision*100}%')\n",
    "print(f'> Recall : {article_brand_recall*100}%')\n",
    "print('-'*5)\n",
    "print(\"Sentiment Analysis:\")\n",
    "print(f'> Precision : {article_sent_precision*100}%')\n",
    "print(f'> Recall : {article_sent_recall*100}%')\n",
    "print('-'*5)\n",
    "print('3. Automated Headlines evaluation')\n",
    "print(f'--> Average similarity scores: {avg_headline_sim_score*100}%')\n",
    "print(\"--> Rough Score:\")\n",
    "for k,v in avg_rogue_scores.items():\n",
    "  print('\\t> {} : {}%'.format(k, round(v*100,2)))\n",
    "print(f'--> BLEU Score: {avg_bleu_scores*100}%')\n",
    "print('-'*70)\n",
    "#----------------------------------------------------------------\n",
    "print(\"=\"*30, \"Tweet\", \"=\"*30)\n",
    "print(\"1. Theme classification evaluation (for mobile tech):\")\n",
    "print(f'> Precision : {tweet_precision*100}%')\n",
    "print(f'> Recall : {tweet_recall*100}%')\n",
    "print(f'> F1 Score: {tweet_f_score*100}%')\n",
    "print('-'*5)\n",
    "print(\"2. Entity based evaluation\")\n",
    "print(\"Brand Identification:\")\n",
    "print(f'> Precision : {tweet_brand_precision*100}%')\n",
    "print(f'> Recall : {tweet_brand_recall*100}%')\n",
    "print('-'*5)\n",
    "print(\"Sentiment Analysis:\")\n",
    "print(f'> Precision : {tweet_sent_precision*100}%')\n",
    "print(f'> Recall : {tweet_sent_recall*100}%')\n",
    "print('-'*70)\n",
    "#----------------------------------------------------------------\n",
    "print(\"=\"*30, \"Overall\", \"=\"*30)\n",
    "print(\"1. Theme classification evaluation (for mobile tech):\")\n",
    "print(f'> Precision : {precision*100}%')\n",
    "print(f'> Recall : {recall*100}%')\n",
    "print(f'> F1 Score: {f_score*100}%')\n",
    "print(f'> Accuracy Score: {acc_score*100}%')\n",
    "print('-'*5)\n",
    "print(\"2. Entity based evaluation\")\n",
    "print(\"Brand Identification:\")\n",
    "print(f'> Precision : {overall_brand_precision*100}%')\n",
    "print(f'> Recall : {overall_brand_recall*100}%')\n",
    "print('-'*5)\n",
    "print(\"Sentiment Analysis:\")\n",
    "print(f'> Precision : {overall_sent_precision*100}%')\n",
    "print(f'> Recall : {overall_sent_recall*100}%')\n",
    "print('3. Automated Headlines evaluation')\n",
    "print(f'--> Average similarity scores: {avg_headline_sim_score*100}%')\n",
    "print(\"--> Rough Score:\")\n",
    "for k,v in avg_rogue_scores.items():\n",
    "  print('\\t> {} : {}%'.format(k, round(v*100,2)))  \n",
    "print(f'--> BLEU Score : {avg_bleu_scores*100}%')\n",
    "print('-'*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7omWxWBjIpH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "evaluation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
